{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration Status Spreadsheet Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook generates the data for the migration tracking spreadsheet.\n",
    "\n",
    "## What it does\n",
    "- Extracts migration data from COLIN Extract database\n",
    "- Retrieves filing information from LEAR database\n",
    "- Retrieves affiliation information from Auth database\n",
    "- Retrieves freeze status and early adopter information from COLIN database\n",
    "- Merges and exports data to Excel format\n",
    "- Composes a batch summary tab indicating migration overview of each batch\n",
    "\n",
    "## Output\n",
    "A formatted Excel spreadsheet tracking corporation migration status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install sqlalchemy>=2.0\n",
    "%pip install oracledb\n",
    "%pip install dotenv\n",
    "%pip install psycopg2-binary\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Configuration\n",
    "\n",
    "Import required libraries and load environment variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError, OperationalError\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "COLUMN_NAMES = {\n",
    "    \"group\": \"Group\",\n",
    "    \"batch\": \"Batch\",\n",
    "    \"email\": \"Admin Email\",\n",
    "    \"corp_num\": \"Incorporation Number\",\n",
    "    \"corp_name\": \"Company Name\",\n",
    "    \"corp_type\": \"Type\",\n",
    "    \"frozen_in_colin\": \"Frozen in COLIN\",\n",
    "    \"banner_updated_in_colin\": \"COLIN Banner Updated\",\n",
    "    \"status\": \"Migration Status\",\n",
    "    \"date\": \"Migrated Date\",\n",
    "    \"affiliated\": \"Affiliated\",\n",
    "    \"account\": \"Account ID\",\n",
    "    \"account_name\": \"Account Name\",\n",
    "    \"filings\": \"Filings Done\",\n",
    "    \"filing_date\": \"Last Filing Date\",\n",
    "    \"legacy_outputs_uploaded_drs\": \"Legacy Outputs Uploaded DRS\",\n",
    "    \"legacy_outputs_document_entries_created\": \"Legacy Outputs Document Entries Created in LEAR\"\n",
    "}\n",
    "\n",
    "SUMMARY_COL_NAMES = {\n",
    "    \"group_display_name\": \"Group\",\n",
    "    \"batch_display_name\": \"Batch\",\n",
    "    \"requested_date\": \"Requested Date\",\n",
    "    \"batch_status\": \"Migration Status\",\n",
    "    \"migrated_date\": \"Migrated Date\",\n",
    "    \"batch_size\": \"Batch Size\",\n",
    "    \"migrated_businesses\": \"Migrated Businesses\",\n",
    "    \"notes\": \"Notes\"\n",
    "}\t\t\t\t\t\n",
    "\n",
    "TAB_NAMES = {\n",
    "    \"status\": \"Migration Status\",\n",
    "    \"summary\": \"Batch Summary\"\n",
    "}\n",
    "\n",
    "CONFIG = {\n",
    "    'batch_size': 5000,\n",
    "    'final_excel_fields': [\n",
    "        COLUMN_NAMES[\"group\"],\n",
    "        COLUMN_NAMES[\"batch\"],\n",
    "        COLUMN_NAMES[\"email\"],\n",
    "        COLUMN_NAMES[\"corp_num\"],\n",
    "        COLUMN_NAMES[\"corp_name\"],\n",
    "        COLUMN_NAMES[\"corp_type\"],\n",
    "        COLUMN_NAMES[\"frozen_in_colin\"],\n",
    "        COLUMN_NAMES[\"banner_updated_in_colin\"],\n",
    "        COLUMN_NAMES[\"status\"],\n",
    "        COLUMN_NAMES[\"date\"],\n",
    "        COLUMN_NAMES[\"affiliated\"],\n",
    "        COLUMN_NAMES[\"account\"],\n",
    "        COLUMN_NAMES['account_name'],\n",
    "        COLUMN_NAMES[\"filings\"],\n",
    "        COLUMN_NAMES[\"filing_date\"],\n",
    "        COLUMN_NAMES[\"legacy_outputs_uploaded_drs\"],\n",
    "        COLUMN_NAMES[\"legacy_outputs_document_entries_created\"]\n",
    "    ],\n",
    "    'excel_export': {\n",
    "        'font_size': 14,\n",
    "        'max_column_width': 65,\n",
    "        'filled_color': 'FFCCCC',\n",
    "        'output_dir': os.getenv('EXPORT_OUTPUT_DIR')\n",
    "    }\n",
    "}\n",
    "\n",
    "MIGRATION_STATUS = {\n",
    "    'COMPLETED': 'COMPLETED',\n",
    "    'FAILED': 'FAILED',\n",
    "    'PENDING': 'PENDING',\n",
    "    'PARTIAL': 'PARTIAL'\n",
    "}\n",
    "\n",
    "FLAG_STATUS = {\n",
    "    'YES': 'Y',\n",
    "    'NO': 'N',\n",
    "    'PARTIAL': 'PARTIAL',\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = CONFIG['batch_size']\n",
    "FINAL_EXCEL_FIELDS = CONFIG['final_excel_fields']\n",
    "MIG_GROUP_IDS = [int(x.strip()) for x in os.getenv('MIG_GROUP_IDS').split(',') if x.strip().isdigit()]\n",
    "\n",
    "if not MIG_GROUP_IDS:\n",
    "    raise ValueError(\"MIG_GROUP_IDS is empty! Need at least one group id.\")\n",
    "\n",
    "mig_group_ids = ','.join(str(x) for x in MIG_GROUP_IDS)\n",
    "\n",
    "ORACLE_SCHEMA = os.getenv('DATABASE_COLIN_ORACLE_SCHEMA')\n",
    "\n",
    "if not ORACLE_SCHEMA:\n",
    "    raise ValueError(\"DATABASE_COLIN_ORACLE_SCHEMA is not set.\")\n",
    "\n",
    "print(\"Libraries imported and configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Configure database connections using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_CONFIG = {\n",
    "    'colin_extract': {\n",
    "        'username': os.getenv(\"DATABASE_COLIN_EXTRACT_USERNAME\"),\n",
    "        'password': os.getenv(\"DATABASE_COLIN_EXTRACT_PASSWORD\"),\n",
    "        'host': os.getenv(\"DATABASE_COLIN_EXTRACT_HOST\"),\n",
    "        'port': os.getenv(\"DATABASE_COLIN_EXTRACT_PORT\"),\n",
    "        'name': os.getenv(\"DATABASE_COLIN_EXTRACT_NAME\")\n",
    "    },\n",
    "    'lear': {\n",
    "        'username': os.getenv(\"DATABASE_LEAR_USERNAME\"),\n",
    "        'password': os.getenv(\"DATABASE_LEAR_PASSWORD\"),\n",
    "        'host': os.getenv(\"DATABASE_LEAR_HOST\"),\n",
    "        'port': os.getenv(\"DATABASE_LEAR_PORT\"),\n",
    "        'name': os.getenv(\"DATABASE_LEAR_NAME\")\n",
    "    },\n",
    "    'auth': {\n",
    "        'username': os.getenv(\"DATABASE_AUTH_USERNAME\"),\n",
    "        'password': os.getenv(\"DATABASE_AUTH_PASSWORD\"),\n",
    "        'host': os.getenv(\"DATABASE_AUTH_HOST\"),\n",
    "        'port': os.getenv(\"DATABASE_AUTH_PORT\"),\n",
    "        'name': os.getenv(\"DATABASE_AUTH_NAME\")\n",
    "    },\n",
    "    'doc': {\n",
    "        'username': os.getenv(\"DATABASE_DOC_USERNAME\"),\n",
    "        'password': os.getenv(\"DATABASE_DOC_PASSWORD\"),\n",
    "        'host': os.getenv(\"DATABASE_DOC_HOST\"),\n",
    "        'port': os.getenv(\"DATABASE_DOC_PORT\"),\n",
    "        'name': os.getenv(\"DATABASE_DOC_NAME\")\n",
    "    },\n",
    "    'colin_oracle': {\n",
    "        'username': os.getenv(\"DATABASE_COLIN_ORACLE_USERNAME\"),\n",
    "        'password': os.getenv(\"DATABASE_COLIN_ORACLE_PASSWORD\"),\n",
    "        'host': os.getenv(\"DATABASE_COLIN_ORACLE_HOST\"),\n",
    "        'port': os.getenv(\"DATABASE_COLIN_ORACLE_PORT\"),\n",
    "        'name': os.getenv(\"DATABASE_COLIN_ORACLE_NAME\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for db_key, db_config in DATABASE_CONFIG.items():\n",
    "    # Build Oracle URI\n",
    "    if db_key == 'colin_oracle':\n",
    "        uri = f\"oracle+oracledb://{db_config['username']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['name']}\"\n",
    "    # Build PostgreSQL URI\n",
    "    else:\n",
    "        uri = f\"postgresql://{db_config['username']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['name']}\"\n",
    "    DATABASE_CONFIG[db_key] = {'uri': uri}\n",
    "\n",
    "print(\"Database configurations successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Database Engines\n",
    "\n",
    "Create and test database connections for all configured databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracledb.init_oracle_client()\n",
    "\n",
    "engines = {}\n",
    "\n",
    "for db_key, config in DATABASE_CONFIG.items():\n",
    "    try:\n",
    "        engine = create_engine(config['uri'])\n",
    "        \n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            if db_key =='colin_oracle':\n",
    "                conn.execute(text(\"SELECT 1 FROM DUAL\"))\n",
    "            else:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "        \n",
    "        engines[db_key] = engine\n",
    "        print(f\"{db_key.upper()} database engine created and tested successfully.\")\n",
    "    \n",
    "    except OperationalError as e:\n",
    "        print(f\"{db_key.upper()} database connection failed: {e}\")\n",
    "        raise\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"{db_key.upper()} database engine creation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"{db_key.upper()} unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "ENGINE_NAMES = {engine: key for key, engine in engines.items()}\n",
    "\n",
    "print(\"All database engines ready for use.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Migration Data\n",
    "\n",
    "Query COLIN Extract database to get list of migrated corporations with their details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colin_extract_query = f\"\"\"\n",
    "SELECT\n",
    "    g.display_name AS \"{COLUMN_NAMES['group']}\",\n",
    "    b.display_name AS \"{COLUMN_NAMES['batch']}\",\n",
    "    mcb.corp_num AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "    c.admin_email AS \"{COLUMN_NAMES['email']}\",\n",
    "    cn.corp_name AS \"{COLUMN_NAMES['corp_name']}\",\n",
    "    c.corp_type_cd AS \"{COLUMN_NAMES['corp_type']}\",\n",
    "    CASE\n",
    "        WHEN cp.processed_status = 'COMPLETED' THEN '{MIGRATION_STATUS['COMPLETED']}'\n",
    "        WHEN cp.processed_status = 'FAILED' THEN '{MIGRATION_STATUS['FAILED']}'\n",
    "        WHEN cp.processed_status IS NULL THEN '{MIGRATION_STATUS['PENDING']}'\n",
    "        ELSE '{MIGRATION_STATUS['PENDING']}'\n",
    "    END AS \"{COLUMN_NAMES['status']}\",\n",
    "    cp.create_date::date AS \"{COLUMN_NAMES['date']}\"\n",
    "FROM\n",
    "    mig_corp_batch mcb\n",
    "    JOIN \n",
    "        mig_batch b ON mcb.mig_batch_id = b.id\n",
    "    JOIN \n",
    "        mig_group g ON b.mig_group_id = g.id\n",
    "    LEFT JOIN \n",
    "        corporation c ON mcb.corp_num = c.corp_num\n",
    "    LEFT JOIN \n",
    "        corp_processing cp ON mcb.corp_num = cp.corp_num\n",
    "    LEFT JOIN \n",
    "        corp_name cn ON c.corp_num = cn.corp_num \n",
    "            AND cn.corp_name_typ_cd IN ('CO', 'NB') \n",
    "            AND cn.end_event_id IS NULL\n",
    "WHERE\n",
    "    g.id IN ({mig_group_ids})\n",
    "    AND (\n",
    "        (cp.processed_status = 'COMPLETED' AND cp.environment = 'prod')\n",
    "        OR (cp.processed_status = 'FAILED' AND cp.environment = 'prod')\n",
    "        OR cp.processed_status IS NULL\n",
    "    )\n",
    "ORDER BY\n",
    "    g.display_name, \n",
    "    b.display_name,\n",
    "    CASE\n",
    "        WHEN cp.processed_status = 'COMPLETED' THEN 0\n",
    "        WHEN cp.processed_status = 'FAILED' THEN 1\n",
    "        ELSE 2\n",
    "    END, \n",
    "    cp.create_date DESC,\n",
    "    cn.corp_name;\n",
    "\"\"\"\n",
    "    \n",
    "try:\n",
    "    with engines['colin_extract'].connect() as conn:\n",
    "        colin_extract_df = pd.read_sql(colin_extract_query, conn)\n",
    "\n",
    "    if colin_extract_df.empty:\n",
    "        raise ValueError(\"COLIN Extract database query returned empty result\")\n",
    "    \n",
    "    print(f\"Fetched {len(colin_extract_df)} rows from COLIN Extract database.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data from COLIN Extract: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(colin_extract_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Query Function\n",
    "A function to perform batch queries across multiple databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_query(query_sql, db_engine, batch_size, columns, is_colin_oracle=False, additional_params=None, dedup=True):\n",
    "    # Get unique corporation numbers from the dataset\n",
    "    unique_corp_nums = colin_extract_df[COLUMN_NAMES['corp_num']].unique().tolist()\n",
    "\n",
    "    if is_colin_oracle:\n",
    "        # Convert corp_nums format if query in COLIN db\n",
    "        corp_num_mapping = {corp_num[2:] if corp_num.startswith('BC') else corp_num: corp_num\n",
    "                               for corp_num in unique_corp_nums}\n",
    "        unique_corp_nums = list(corp_num_mapping.keys())\n",
    "    else:\n",
    "        corp_num_mapping = None\n",
    "\n",
    "    corp_number_batches = [unique_corp_nums[i:i + batch_size] for i in range(0, len(unique_corp_nums), batch_size)]\n",
    "    db_name = ENGINE_NAMES.get(db_engine, \"Unknown database\")\n",
    "    batch_results = []\n",
    "    \n",
    "    # Process each batch of corporation numbers\n",
    "    for batch_idx, current_batch_corp_numbers in enumerate(corp_number_batches):\n",
    "        if not current_batch_corp_numbers:\n",
    "            continue\n",
    "        try:\n",
    "            with db_engine.connect() as conn:\n",
    "                if is_colin_oracle:\n",
    "                    corp_nums_str = ', '.join([f\"'{x}'\" for x in current_batch_corp_numbers])\n",
    "                    actual_query = query_sql.replace('{identifiers}', corp_nums_str)\n",
    "                    df = pd.read_sql(actual_query, conn)\n",
    "                else:\n",
    "                    params = {'identifiers': current_batch_corp_numbers}\n",
    "                    if additional_params:\n",
    "                        params.update(additional_params)\n",
    "\n",
    "                    df = pd.read_sql(query_sql, conn, params=params)\n",
    "            \n",
    "            # Store results from this batch\n",
    "            batch_results.append(df)\n",
    "            print(f\"{db_name} Batch {batch_idx+1}: {len(df)} records fetched\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"{db_name} Batch {batch_idx+1}/{len(corp_number_batches)} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Process combined results\n",
    "    if batch_results:\n",
    "        combined_df = pd.concat(batch_results, ignore_index=True)\n",
    "\n",
    "        # Convert back to corp format starts with BC\n",
    "        if is_colin_oracle and corp_num_mapping:\n",
    "            combined_df[COLUMN_NAMES['corp_num']] = combined_df[COLUMN_NAMES['corp_num']].map(corp_num_mapping)\n",
    "\n",
    "        if dedup:\n",
    "            combined_df = combined_df.drop_duplicates(COLUMN_NAMES['corp_num'], keep='last')\n",
    "        print(f\"Total records fetched: {len(combined_df)}\")\n",
    "    else:\n",
    "        combined_df = pd.DataFrame(columns=columns)\n",
    "        print(f\"No records fetched\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Filing Data\n",
    "\n",
    "Retrieve and aggregate filing information from LEAR database for migrated corporations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lear_combined_query = f\"\"\"\n",
    "SELECT \n",
    "    b.id,\n",
    "    b.identifier AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "    COALESCE(\n",
    "        STRING_AGG(f.filing_type, ', ' ORDER BY f.filing_type), \n",
    "        ''\n",
    "    ) AS \"{COLUMN_NAMES['filings']}\",\n",
    "    MAX(f.filing_date)::date AS \"{COLUMN_NAMES['filing_date']}\"\n",
    "FROM businesses b\n",
    "LEFT JOIN filings f ON b.id = f.business_id \n",
    "    AND f.source = 'LEAR' \n",
    "    AND f.status = 'COMPLETED'\n",
    "WHERE b.identifier = ANY(%(identifiers)s)\n",
    "GROUP BY b.id, b.identifier;\n",
    "\"\"\"\n",
    "\n",
    "lear_combined_df = batch_query(\n",
    "    query_sql=lear_combined_query,\n",
    "    db_engine=engines['lear'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=['id', COLUMN_NAMES['corp_num'], COLUMN_NAMES[\"filings\"], COLUMN_NAMES[\"filing_date\"]]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(lear_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_fill_missing_data(colin_extract_df, source_df, source_name, default_values):\n",
    "    \"\"\"Data validation and default value filling method\"\"\"\n",
    "    \n",
    "    # Get all non-pending status corporations\n",
    "    non_pending_corps = colin_extract_df[\n",
    "        colin_extract_df[COLUMN_NAMES['status']] != MIGRATION_STATUS['PENDING']\n",
    "    ][COLUMN_NAMES['corp_num']].tolist()\n",
    "\n",
    "    # Get corporation list from source database\n",
    "    source_corps = source_df[COLUMN_NAMES['corp_num']].tolist()\n",
    "    missing_corps = set(non_pending_corps) - set(source_corps)\n",
    "\n",
    "    if missing_corps:\n",
    "        print(f\" {len(missing_corps)} corporations missing from {source_name} database\")\n",
    "\n",
    "        # Create default records for missing corporations\n",
    "        missing_records = []\n",
    "        for corp_num in missing_corps:\n",
    "            record = {COLUMN_NAMES['corp_num']: corp_num}\n",
    "            record.update(default_values)\n",
    "            missing_records.append(record)\n",
    "\n",
    "        if missing_records:\n",
    "            missing_df = pd.DataFrame(missing_records)\n",
    "            source_df = pd.concat([source_df, missing_df], ignore_index=True)\n",
    "\n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Affiliation Data\n",
    "\n",
    "Query the Auth database to get affiliation information, including whether corporations are affiliated and their account IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_query = f\"\"\"\n",
    "SELECT\n",
    "    e.business_identifier AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "    CASE WHEN COUNT(a.id) > 0 THEN '{FLAG_STATUS['YES']}' ELSE '{FLAG_STATUS['NO']}' END AS \"{COLUMN_NAMES['affiliated']}\",\n",
    "    COALESCE(\n",
    "        STRING_AGG(a.org_id::text, ', ' ORDER BY a.org_id),\n",
    "        ''\n",
    "    ) AS \"{COLUMN_NAMES['account']}\",\n",
    "    COALESCE(\n",
    "        STRING_AGG(o.name, ', ' ORDER BY a.org_id),\n",
    "        ''\n",
    "    ) AS \"{COLUMN_NAMES['account_name']}\"\n",
    "FROM\n",
    "    entities e\n",
    "LEFT JOIN\n",
    "    affiliations a ON e.id = a.entity_id \n",
    "LEFT JOIN\n",
    "    orgs o ON a.org_id = o.id\n",
    "WHERE\n",
    "    e.business_identifier = ANY(%(identifiers)s)\n",
    "GROUP BY\n",
    "    e.business_identifier\n",
    "\"\"\"\n",
    "\n",
    "auth_combined_df = batch_query(\n",
    "    query_sql=auth_query,\n",
    "    db_engine=engines['auth'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[COLUMN_NAMES['corp_num'], COLUMN_NAMES['affiliated'], COLUMN_NAMES['account'], COLUMN_NAMES['account_name']]\n",
    ")\n",
    "\n",
    "AUTH_DEFAULT_VALUES = {\n",
    "    COLUMN_NAMES['affiliated']: FLAG_STATUS['NO'],\n",
    "    COLUMN_NAMES['account']: '',\n",
    "    COLUMN_NAMES['account_name']: ''\n",
    "}\n",
    " \n",
    "# Auth data validation and filling\n",
    "auth_combined_df = validate_and_fill_missing_data(\n",
    "    colin_extract_df,\n",
    "    auth_combined_df,\n",
    "    'Auth',\n",
    "    AUTH_DEFAULT_VALUES\n",
    ")\n",
    "\n",
    "# Display results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(auth_combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get COLIN Data\n",
    "\n",
    "Retrieve corporation freeze status and early adopter information from Oracle COLIN database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colin_oracle_query = f\"\"\"\n",
    "SELECT\n",
    "    c.corp_num AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "    CASE WHEN c.CORP_FROZEN_TYP_CD = 'C' THEN '{FLAG_STATUS['YES']}' ELSE '{FLAG_STATUS['NO']}' END AS \"{COLUMN_NAMES['frozen_in_colin']}\",\n",
    "    CASE WHEN cea.corp_num IS NOT NULL THEN '{FLAG_STATUS['YES']}' ELSE '{FLAG_STATUS['NO']}' END AS \"{COLUMN_NAMES['banner_updated_in_colin']}\"\n",
    "FROM\n",
    "    {ORACLE_SCHEMA}.CORPORATION c\n",
    "    LEFT JOIN {ORACLE_SCHEMA}.CORP_EARLY_ADOPTERS cea ON c.corp_num = cea.corp_num\n",
    "WHERE\n",
    "    c.corp_num IN ({{identifiers}})\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "colin_oracle_combined_df = batch_query(\n",
    "    query_sql=colin_oracle_query,\n",
    "    db_engine=engines['colin_oracle'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[COLUMN_NAMES['corp_num'], COLUMN_NAMES['frozen_in_colin'], COLUMN_NAMES['banner_updated_in_colin']],\n",
    "    is_colin_oracle=True\n",
    ")\n",
    "\n",
    "COLIN_DEFAULT_VALUES = {\n",
    "    COLUMN_NAMES['frozen_in_colin']: FLAG_STATUS['NO'],\n",
    "    COLUMN_NAMES['banner_updated_in_colin']: FLAG_STATUS['NO'],\n",
    "}\n",
    " \n",
    "# Colin data validation and filling\n",
    "colin_oracle_combined_df = validate_and_fill_missing_data(\n",
    "    colin_extract_df,\n",
    "    colin_oracle_combined_df,\n",
    "    'COLIN',\n",
    "    COLIN_DEFAULT_VALUES\n",
    ")\n",
    "\n",
    "# Display results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(colin_oracle_combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Legacy Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get colin_event_ids and file_keys for migrated corps from LEAR\n",
    "lear_colin_event_detail_query = f\"\"\"\n",
    "SELECT \n",
    "    b.identifier AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "    cei.colin_event_id,\n",
    "    d.file_key\n",
    "FROM businesses b\n",
    "JOIN filings f ON b.id = f.business_id\n",
    "JOIN colin_event_ids cei ON f.id = cei.filing_id\n",
    "LEFT JOIN documents d ON f.id = d.filing_id\n",
    "WHERE b.identifier = ANY(%(identifiers)s)\n",
    "AND f.source = 'COLIN'\n",
    "AND f.filing_type != 'lear_tombstone'\n",
    "\"\"\"\n",
    "\n",
    "lear_colin_event_detail_df = batch_query(\n",
    "    query_sql=lear_colin_event_detail_query,\n",
    "    db_engine=engines['lear'],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    columns=[COLUMN_NAMES['corp_num'], 'colin_event_id', 'file_key'],\n",
    "    dedup=False\n",
    ")\n",
    "\n",
    "# Display LEAR query results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(lear_colin_event_detail_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query DRS, match both entity_id and event_id\n",
    "drs_detail_query = f\"\"\"\n",
    "    SELECT \n",
    "        ar.entity_id AS \"{COLUMN_NAMES['corp_num']}\",\n",
    "        ar.event_id,\n",
    "        ar.document_service_id\n",
    "    FROM application_reports ar\n",
    "    WHERE ar.entity_id = ANY(%(identifiers)s)\n",
    "    AND ar.event_id = ANY(%(event_ids)s)\n",
    "    \"\"\"\n",
    "\n",
    "if not lear_colin_event_detail_df.empty:\n",
    "    colin_event_ids = lear_colin_event_detail_df['colin_event_id'].unique().tolist()\n",
    "    corp_nums = lear_colin_event_detail_df[COLUMN_NAMES['corp_num']].unique().tolist()\n",
    "    \n",
    "    drs_detail_df = batch_query(\n",
    "        query_sql=drs_detail_query,\n",
    "        db_engine=engines['doc'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        columns=[COLUMN_NAMES['corp_num'], 'event_id', 'document_service_id'],\n",
    "        additional_params={'event_ids': colin_event_ids},\n",
    "        dedup=False\n",
    "    )\n",
    "\n",
    "    # Display DRS query results\n",
    "    print(f\"Fetched {len(drs_detail_df)} rows from Doc database for DRS legacy outputs.\")\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(drs_detail_df)\n",
    "else:\n",
    "    drs_detail_df = pd.DataFrame(columns=[COLUMN_NAMES['corp_num'], 'event_id', 'document_service_id'])\n",
    "    print(\"No colin event data found in LEAR, skipping DRS query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_legacy_status_logic(corp_num, corp_colin_event_lear, corp_drs):\n",
    "    # DRS legacy outputs status calculation\n",
    "    if corp_drs.empty:\n",
    "        drs_status = FLAG_STATUS['NO']\n",
    "    else:\n",
    "        if corp_colin_event_lear.empty:\n",
    "            corp_colin_event_ids = set()\n",
    "        else:\n",
    "            # Get all unique colin_event_ids for this corporation\n",
    "            corp_colin_event_ids = set(corp_colin_event_lear['colin_event_id'].tolist())\n",
    "\n",
    "        # Get all unique event_ids from DRS for this corporation\n",
    "        corp_drs_event_ids = set(corp_drs['event_id'].tolist())\n",
    "\n",
    "        if len(corp_drs_event_ids) == 0:\n",
    "            drs_status = FLAG_STATUS['NO']\n",
    "        elif corp_drs_event_ids == corp_colin_event_ids:\n",
    "            drs_status = FLAG_STATUS['YES']\n",
    "        else:\n",
    "            drs_status = FLAG_STATUS['PARTIAL']\n",
    "\n",
    "    # LEAR legacy outputs status calculation\n",
    "    if corp_colin_event_lear.empty:\n",
    "        lear_status = FLAG_STATUS['NO']\n",
    "    else:\n",
    "        # Get all unique file keys from LEAR documents for this corporation (filter NULL values)\n",
    "        lear_file_keys = set(corp_colin_event_lear['file_key'].dropna().tolist())\n",
    "\n",
    "        # Determine LEAR legacy outputs documents creation status\n",
    "        if drs_status == FLAG_STATUS['NO']:\n",
    "            lear_status = FLAG_STATUS['NO']\n",
    "\n",
    "            # Log data inconsistency if found\n",
    "            if len(lear_file_keys) > 0:\n",
    "                print(f\"DATA QUALITY ISSUE: Corp {corp_num} - DRS=N but LEAR has {len(lear_file_keys)} document entries\")\n",
    "\n",
    "        else:\n",
    "            # Get all unique document_service_ids from DRS (filter NULL values)\n",
    "            drs_document_service_ids = set(corp_drs['document_service_id'].dropna().tolist())\n",
    " \n",
    "            # Calculate matching between DRS and LEAR\n",
    "            matched_keys = lear_file_keys.intersection(drs_document_service_ids)\n",
    "            total_drs_keys = len(drs_document_service_ids)\n",
    "            matched_count = len(matched_keys)\n",
    "\n",
    "            if drs_status == FLAG_STATUS['YES']:\n",
    "                if matched_count == total_drs_keys:\n",
    "                    lear_status = FLAG_STATUS['YES']\n",
    "                elif matched_count == 0:\n",
    "                    lear_status = FLAG_STATUS['NO']\n",
    "                else:\n",
    "                    lear_status = FLAG_STATUS['PARTIAL']\n",
    "            else:  # PARTIAL\n",
    "                if matched_count > 0:\n",
    "                    lear_status = FLAG_STATUS['PARTIAL']\n",
    "                else:\n",
    "                    lear_status = FLAG_STATUS['NO']\n",
    "\n",
    "    return drs_status, lear_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corp_legacy_outputs_status():\n",
    "    processed_corps = colin_extract_df[\n",
    "        colin_extract_df[COLUMN_NAMES['status']] != MIGRATION_STATUS['PENDING']\n",
    "    ][COLUMN_NAMES['corp_num']].unique().tolist()\n",
    "\n",
    "    if not processed_corps:\n",
    "        print(\"No non-pending corporations found\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            COLUMN_NAMES['corp_num'],\n",
    "            COLUMN_NAMES['legacy_outputs_uploaded_drs'],\n",
    "            COLUMN_NAMES['legacy_outputs_document_entries_created']\n",
    "        ])\n",
    "\n",
    "    # pre-grouping corps legacy outputs data \n",
    "    lear_colin_event_grouped = lear_colin_event_detail_df.groupby(COLUMN_NAMES['corp_num'])\n",
    "    drs_grouped = drs_detail_df.groupby(COLUMN_NAMES['corp_num'])\n",
    "    lear_colin_event_corps = set(lear_colin_event_grouped.groups.keys())\n",
    "    drs_corps = set(drs_grouped.groups.keys())\n",
    "\n",
    "    # create empty DF\n",
    "    empty_lear_df = pd.DataFrame(columns=lear_colin_event_detail_df.columns)\n",
    "    empty_drs_df = pd.DataFrame(columns=drs_detail_df.columns)\n",
    "\n",
    "\n",
    "    result = []\n",
    "    for corp_num in processed_corps:\n",
    "        try:\n",
    "            corp_colin_event_lear = lear_colin_event_grouped.get_group(corp_num) if corp_num in lear_colin_event_corps else empty_lear_df\n",
    "            corp_drs = drs_grouped.get_group(corp_num) if corp_num in drs_corps else empty_drs_df\n",
    "\n",
    "            # calculate legacy outputs status for each grouped corp\n",
    "            drs_status, lear_status = calculate_legacy_status_logic(corp_num, corp_colin_event_lear, corp_drs)\n",
    "\n",
    "            result.append({\n",
    "                COLUMN_NAMES['corp_num']: corp_num,\n",
    "                COLUMN_NAMES['legacy_outputs_uploaded_drs']: drs_status,\n",
    "                COLUMN_NAMES['legacy_outputs_document_entries_created']: lear_status\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing corporation {corp_num}: {e}\")\n",
    "            result.append({\n",
    "                COLUMN_NAMES['corp_num']: corp_num,\n",
    "                COLUMN_NAMES['legacy_outputs_uploaded_drs']: '',\n",
    "                COLUMN_NAMES['legacy_outputs_document_entries_created']: ''\n",
    "            })\n",
    "\n",
    "    print(f\"Processing completed. Generated status for {len(result)} corporations.\")\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "legacy_outputs_df = calculate_corp_legacy_outputs_status()\n",
    "\n",
    "# Data validation and filling\n",
    "LEGACY_OUTPUTS_DEFAULT_VALUES = {\n",
    "    COLUMN_NAMES['legacy_outputs_uploaded_drs']: FLAG_STATUS['NO'],\n",
    "    COLUMN_NAMES['legacy_outputs_document_entries_created']: FLAG_STATUS['NO']\n",
    "}\n",
    "\n",
    "legacy_outputs_df = validate_and_fill_missing_data(\n",
    "    colin_extract_df,\n",
    "    legacy_outputs_df,\n",
    "    'Legacy Outputs',\n",
    "    LEGACY_OUTPUTS_DEFAULT_VALUES\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Generated legacy outputs status for {len(legacy_outputs_df)} corporations.\")\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(legacy_outputs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data\n",
    "\n",
    "Combine data from COLIN Extract, LEAR, and Auth databases into a merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = (colin_extract_df\n",
    "              .merge(lear_combined_df, \n",
    "                     on=COLUMN_NAMES['corp_num'], \n",
    "                     how='left')\n",
    "              .merge(auth_combined_df,\n",
    "                     on=COLUMN_NAMES['corp_num'],\n",
    "                     how='left')\n",
    "              .merge(colin_oracle_combined_df,\n",
    "                     on=COLUMN_NAMES['corp_num'],\n",
    "                     how='left')\n",
    "              .merge(legacy_outputs_df,\n",
    "                     on=COLUMN_NAMES['corp_num'],\n",
    "                     how='left')\n",
    "              )\n",
    "    \n",
    "    # Select final fields\n",
    "    merged_df = result[FINAL_EXCEL_FIELDS]\n",
    "    \n",
    "    print(f\"Data merged successfully: {len(merged_df)} rows\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error merging data: {e}\")\n",
    "\n",
    "# Display merged results\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Batch Summary Dataframe\n",
    "Query Colin Extract database to compose the Batch Summary tab.\n",
    "<br />Currently including 8 columns: Group, Batch, Requested Date, Migration Status, Migrated Date, Batch Size, Migrated Businesses, Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_summary_query = f\"\"\"\n",
    "WITH batch_status AS (\n",
    "            SELECT \n",
    "                b.id as batch_id,\n",
    "                g.display_name as group_display_name,\n",
    "                b.display_name as batch_display_name,\n",
    "                b.requested_date,\n",
    "                b.migrated_date,\n",
    "                b.notes,\n",
    "                COUNT(DISTINCT mcb.corp_num) as batch_size,\n",
    "                COUNT(DISTINCT CASE \n",
    "                    WHEN cp.processed_status = 'COMPLETED' AND cp.environment = 'prod'\n",
    "                    THEN mcb.corp_num \n",
    "                END) as completed_corps,\n",
    "                COUNT(DISTINCT CASE \n",
    "                    WHEN cp.corp_num IS NOT NULL AND cp.environment = 'prod'\n",
    "                    THEN mcb.corp_num \n",
    "                END) as has_processing_records\n",
    "            FROM mig_group g\n",
    "            JOIN mig_batch b ON g.id = b.mig_group_id\n",
    "            LEFT JOIN mig_corp_batch mcb ON b.id = mcb.mig_batch_id\n",
    "            LEFT JOIN corp_processing cp ON mcb.corp_num = cp.corp_num\n",
    "            WHERE g.id IN ({mig_group_ids})\n",
    "            GROUP BY b.id, g.display_name, b.display_name, b.requested_date, b.migrated_date\n",
    "        )\n",
    "        SELECT \n",
    "            group_display_name,\n",
    "            batch_display_name,\n",
    "            requested_date,\n",
    "            migrated_date,\n",
    "            batch_size,\n",
    "            completed_corps as migrated_businesses,\n",
    "            notes,\n",
    "            CASE\n",
    "                WHEN has_processing_records = 0 THEN '{MIGRATION_STATUS['PENDING']}'\n",
    "                WHEN batch_size = completed_corps THEN '{MIGRATION_STATUS['COMPLETED']}'\n",
    "                WHEN completed_corps = 0 AND has_processing_records > 0 THEN '{MIGRATION_STATUS['FAILED']}'\n",
    "                WHEN completed_corps > 0 AND completed_corps < batch_size THEN '{MIGRATION_STATUS['PARTIAL']}'\n",
    "                ELSE '{MIGRATION_STATUS['PENDING']}'\n",
    "            END as batch_status\n",
    "        FROM batch_status\n",
    "        ORDER BY group_display_name, batch_display_name\n",
    "\"\"\"\n",
    "try:\n",
    "    with engines['colin_extract'].connect() as conn:\n",
    "        batch_summary_df = pd.read_sql(batch_summary_query, conn)\n",
    "    \n",
    "    if batch_summary_df.empty:\n",
    "        raise ValueError(\"batch summary data query returned 0 result\")\n",
    "    \n",
    "    print(f\"Composed {len(batch_summary_df)} entries for batch summary\")\n",
    "\n",
    "    # formatting the dataframe with proper column order and column names\n",
    "    column_order = ['group_display_name', 'batch_display_name', 'requested_date', 'batch_status', 'migrated_date', 'batch_size', 'migrated_businesses', 'notes']\n",
    "    batch_summary_df = batch_summary_df[column_order]\n",
    "    batch_summary_df = batch_summary_df.rename(columns=SUMMARY_COL_NAMES)\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data to compose batch summary: {e}\")\n",
    "    raise\n",
    "\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(batch_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Excel\n",
    "\n",
    "Generate formatted Excel file with the merged migration tracking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define highlighting rules for Migration Status tab\n",
    "MIGRATION_STATUS_HIGHLIGHTING_RULES = [\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['affiliated'],\n",
    "        'condition_value': FLAG_STATUS['NO'],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'additional_condition': {\n",
    "            'column': COLUMN_NAMES['status'],\n",
    "            'values': [MIGRATION_STATUS['COMPLETED'], MIGRATION_STATUS['FAILED']]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['banner_updated_in_colin'], \n",
    "        'condition_value': FLAG_STATUS['NO'],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'additional_condition': {\n",
    "            'column': COLUMN_NAMES['status'],\n",
    "            'values': [MIGRATION_STATUS['COMPLETED'], MIGRATION_STATUS['FAILED']]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['status'],\n",
    "        'condition_value': MIGRATION_STATUS['FAILED'], \n",
    "        'fill_color': CONFIG['excel_export']['filled_color']\n",
    "    },\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['frozen_in_colin'],\n",
    "        'condition_value': FLAG_STATUS['NO'],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'additional_condition': {\n",
    "            'column': COLUMN_NAMES['status'],\n",
    "            'values': [MIGRATION_STATUS['COMPLETED'], MIGRATION_STATUS['FAILED']]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['legacy_outputs_uploaded_drs'],\n",
    "        'condition_value': [FLAG_STATUS['NO'], FLAG_STATUS['PARTIAL']],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'additional_condition': {\n",
    "            'column': COLUMN_NAMES['status'],\n",
    "            'values': [MIGRATION_STATUS['COMPLETED'], MIGRATION_STATUS['FAILED']]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'column_name': COLUMN_NAMES['legacy_outputs_document_entries_created'],\n",
    "        'condition_value': [FLAG_STATUS['NO'], FLAG_STATUS['PARTIAL']],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'additional_condition': {\n",
    "            'column': COLUMN_NAMES['status'],\n",
    "            'values': [MIGRATION_STATUS['COMPLETED'], MIGRATION_STATUS['FAILED']]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define highlighting rules for Batch Summary tab\n",
    "BATCH_SUMMARY_HIGHLIGHTING_RULES = [\n",
    "    {\n",
    "        'column_name': SUMMARY_COL_NAMES['batch_status'],\n",
    "        'condition_value': MIGRATION_STATUS['PARTIAL'],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'highlight_related_columns': [\n",
    "            SUMMARY_COL_NAMES['migrated_businesses']\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'column_name': SUMMARY_COL_NAMES['batch_status'],\n",
    "        'condition_value': MIGRATION_STATUS['FAILED'],\n",
    "        'fill_color': CONFIG['excel_export']['filled_color'],\n",
    "        'highlight_related_columns': [\n",
    "            SUMMARY_COL_NAMES['migrated_businesses']\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "\n",
    "def apply_cell_highlighting(worksheet, highlighting_rules):\n",
    "    \"\"\"\n",
    "    Apply conditional highlighting to worksheet cells based on rules.\n",
    "    \n",
    "    Args:\n",
    "        worksheet: The openpyxl worksheet\n",
    "        highlighting_rules: List of dicts with column_name, condition_value, fill_color\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of cells highlighted\n",
    "    \"\"\"\n",
    "    highlighted_count = 0\n",
    "    \n",
    "    # Find column indices for all highlighting rules\n",
    "    all_column_names = set()\n",
    "    for rule in highlighting_rules:\n",
    "        all_column_names.add(rule['column_name'])\n",
    "        if 'highlight_related_columns' in rule:\n",
    "            all_column_names.update(rule['highlight_related_columns'])\n",
    "    column_indices = {}\n",
    "    for col_idx, cell in enumerate(worksheet[1], 1):\n",
    "        if cell.value in all_column_names:\n",
    "            column_indices[cell.value] = col_idx\n",
    "    \n",
    "    # Apply highlighting based on rules\n",
    "    for row_num, row in enumerate(worksheet.iter_rows(), 1):\n",
    "        if row_num == 1:  # Skip header row\n",
    "            continue\n",
    "            \n",
    "        for col_idx, cell in enumerate(row, 1):\n",
    "            # Check each highlighting rule\n",
    "            for rule in highlighting_rules:\n",
    "                condition_values = rule['condition_value']\n",
    "                if isinstance(condition_values, str):\n",
    "                    condition_values = [condition_values]\n",
    "\n",
    "                if col_idx == column_indices.get(rule['column_name']) and cell.value in condition_values:\n",
    "                    should_highlight = True\n",
    "                    \n",
    "                    if 'additional_condition' in rule:\n",
    "                        additional_col_idx = column_indices.get(rule['additional_condition']['column'])\n",
    "                        if additional_col_idx:\n",
    "                            additional_cell = row[additional_col_idx - 1]\n",
    "                            if additional_cell.value not in rule['additional_condition']['values']:\n",
    "                                should_highlight = False\n",
    "                    \n",
    "                    if should_highlight:\n",
    "                        fill = PatternFill(start_color=rule['fill_color'], end_color=rule['fill_color'], fill_type='solid')\n",
    "                        cell.fill = fill\n",
    "                        highlighted_count += 1\n",
    "                    \n",
    "                    if 'highlight_related_columns' in rule:\n",
    "                        for related_col_name in rule['highlight_related_columns']:\n",
    "                            if related_col_name in column_indices:\n",
    "                                related_col_idx = column_indices[related_col_name]\n",
    "                                related_cell = row[related_col_idx - 1]\n",
    "                                related_cell.fill = fill\n",
    "                                highlighted_count += 1\n",
    "    \n",
    "    return highlighted_count\n",
    "\n",
    "\n",
    "def format_worksheet(worksheet) -> None:\n",
    "    \"\"\"Format the given worksheet.\"\"\"\n",
    "    \n",
    "    # Define display styles\n",
    "    header_font = Font(size=CONFIG['excel_export']['font_size'], bold=True)\n",
    "    normal_font = Font(size=CONFIG['excel_export']['font_size'])\n",
    "\n",
    "    # Apply cell highlighting based on worksheet type\n",
    "    if worksheet.title == TAB_NAMES['summary']:\n",
    "        # Batch Summary tab\n",
    "        highlighted_count = apply_cell_highlighting(worksheet, BATCH_SUMMARY_HIGHLIGHTING_RULES)\n",
    "    else:\n",
    "        # Migration Status tab\n",
    "        highlighted_count = apply_cell_highlighting(worksheet, MIGRATION_STATUS_HIGHLIGHTING_RULES)\n",
    "\n",
    "    # Format rows (excluding highlighting which is now handled separately)\n",
    "    for row_num, row in enumerate(worksheet.iter_rows(), 1):\n",
    "        for col_idx, cell in enumerate(row, 1):\n",
    "            if row_num == 1:\n",
    "                # Header row\n",
    "                cell.font = header_font\n",
    "            else:\n",
    "                # Data rows\n",
    "                cell.font = normal_font\n",
    "                cell.alignment = Alignment(horizontal='left')\n",
    "    \n",
    "    # Freeze header row\n",
    "    worksheet.freeze_panes = 'A2'\n",
    "\n",
    "    # Add filter\n",
    "    worksheet.auto_filter.ref = worksheet.dimensions\n",
    "    \n",
    "    # Add last updated at top right\n",
    "    last_updated = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    worksheet.cell(row=1, column=worksheet.max_column + 1, value=f\"Last Updated: {last_updated}\").font = normal_font\n",
    "    \n",
    "    # Adjust column width\n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        worksheet.column_dimensions[column_letter].alignment = Alignment(horizontal='left')\n",
    "\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if cell.value and len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except (TypeError, AttributeError):\n",
    "                continue\n",
    "        \n",
    "        adjusted_width = min(max_length + 12, CONFIG['excel_export']['max_column_width'])\n",
    "        worksheet.column_dimensions[column_letter].width = adjusted_width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df.empty:\n",
    "    raise ValueError(\"Data is empty, cannot export\")\n",
    "\n",
    "if batch_summary_df.empty:\n",
    "    raise ValueError(\"Batch Summary dataframe is empty, nothing to export\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['excel_export']['output_dir'], exist_ok=True)\n",
    "\n",
    "# Generate filename\n",
    "# if reading an existing Excel file and update data\n",
    "read_path = os.getenv('READ_FILE_DIR')\n",
    "read_file = os.getenv('EXCEL_FILE_READ')\n",
    "writer_mode = 'create'\n",
    "\n",
    "if not read_file or not read_path:\n",
    "    excel_filename = f\"migration_status.xlsx\"\n",
    "    excel_filepath = os.path.join(CONFIG['excel_export']['output_dir'], excel_filename)\n",
    "    print(\"No file to read. Or reading file path not configured. Creating migration tracking spreadsheet.\")\n",
    "elif os.path.exists(excel_filepath := os.path.join(read_path, read_file)):\n",
    "    writer_mode = 'update'\n",
    "    print(\"Updating migration tracking spreadsheet.\")\n",
    "else:\n",
    "    raise FileExistsError(\"Configured file reading path, but file doesn't exist.\")\n",
    "\n",
    "try:\n",
    "    writer_kwargs = {'engine': 'openpyxl'}\n",
    "    if writer_mode != 'create':\n",
    "        writer_kwargs.update({'mode': 'a', 'if_sheet_exists': 'replace'})\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filepath, **writer_kwargs) as writer:\n",
    "        print(f\"Mode: {writer_mode}\")\n",
    "        # Export Batch Summary tab data\n",
    "        batch_summary_df.to_excel(writer, sheet_name=TAB_NAMES['summary'], index=False)\n",
    "        b_sum_worksheet = writer.sheets[TAB_NAMES['summary']]\n",
    "        format_worksheet(b_sum_worksheet)\n",
    "\n",
    "        # Export Migration Status tab data\n",
    "        merged_df.to_excel(writer, sheet_name=TAB_NAMES['status'], index=False)\n",
    "        mig_status_worksheet = writer.sheets[TAB_NAMES['status']]\n",
    "        format_worksheet(mig_status_worksheet)\n",
    "\n",
    "    print(f\"Excel export successful: {excel_filepath}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Excel export failed: {e}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
